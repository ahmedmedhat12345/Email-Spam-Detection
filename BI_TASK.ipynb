{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "import contractions  # For expanding contractions\n",
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.utils.helpers import download_statistics\n",
        "download_statistics()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPsJVx1VQygk",
        "outputId": "e1496fce-bc55-420c-a158-5788f3a2e3b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word statistics files not found!\n",
            "Downloading... done!\n",
            "Unpacking... done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Adm9qWf1QnFS",
        "outputId": "50f3c232-99f2-4843-d82a-17c23e6152e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading english - 1grams ...\n",
            "generating cache file for faster loading...\n",
            "reading ngrams /root/.ekphrasis/stats/english/counts_1grams.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
            "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
          ]
        }
      ],
      "source": [
        "# Initialize Ekphrasis for slang normalization only.\n",
        "# Pass a callable tokenizer such as str.split instead of a string.\n",
        "slang_processor = TextPreProcessor(\n",
        "    normalize=[],  # No extra normalization for entities\n",
        "    annotate=set(),  # No annotations\n",
        "    unpack_contractions=False,  # Already handling contractions separately\n",
        "    tokenizer=str.split  # Use a simple word-based tokenizer\n",
        ")\n",
        "\n",
        "\n",
        "def normalize_slang(text):\n",
        "    # Process text with Ekphrasis to handle slang normalization.\n",
        "    return \" \".join(slang_processor.pre_process_doc(text))\n",
        "\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('/content/data_spam.csv', encoding='latin-1')\n",
        "df = df[['v1', 'v2']].rename(columns={'v1': 'label', 'v2': 'text'})\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # 1. Remove URLs\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text, flags=re.IGNORECASE)\n",
        "\n",
        "    # 2. Expand contractions\n",
        "    text = contractions.fix(text)\n",
        "\n",
        "    # 3. Replace slang/abbreviations using Ekphrasis for slang normalization\n",
        "    text = normalize_slang(text)\n",
        "\n",
        "    # 4. Handle special characters that cause concatenation (e.g., replace '/' and '.' with spaces)\n",
        "    text = re.sub(r'([/\\.])', r' ', text)\n",
        "\n",
        "    # 5. Normalize whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # 6. Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # 7. Remove noise: numbers and special characters (keep letters and whitespace)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
        "\n",
        "    # 8. Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # 9. Fix repeated characters (e.g., \"soooo\" -> \"sooo\")\n",
        "    tokens = [re.sub(r'(.)\\1{2,}', r'\\1\\1', word) for word in tokens]\n",
        "\n",
        "    # 10. POS tagging\n",
        "    tagged = pos_tag(tokens)\n",
        "\n",
        "    # 11. Case folding (preserve proper nouns initially)\n",
        "    processed, pos_tags = [], []\n",
        "    for word, tag in tagged:\n",
        "        if tag in ['NNP', 'NNPS']:\n",
        "            processed.append(word)\n",
        "        else:\n",
        "            processed.append(word.lower())\n",
        "        pos_tags.append(tag)\n",
        "\n",
        "    # 12. Stop word removal (keep proper nouns)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered, filtered_pos = [], []\n",
        "    for word, tag in zip(processed, pos_tags):\n",
        "        if tag in ['NNP', 'NNPS'] or word not in stop_words:\n",
        "            filtered.append(word)\n",
        "            filtered_pos.append(tag)\n",
        "\n",
        "    # 13. Lemmatization using POS mapping\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    pos_map = {'NN': 'n', 'NNS': 'n', 'VB': 'v', 'VBD': 'v', 'VBG': 'v', 'VBN': 'v',\n",
        "               'VBP': 'v', 'VBZ': 'v', 'JJ': 'a', 'JJR': 'a', 'JJS': 'a', 'RB': 'r'}\n",
        "    lemmas = []\n",
        "    for word, tag in zip(filtered, filtered_pos):\n",
        "        pos = pos_map.get(tag[:2], 'n')\n",
        "        lemmas.append(lemmatizer.lemmatize(word, pos=pos))\n",
        "\n",
        "    # 14. Final lowercase conversion\n",
        "    return ' '.join(word.lower() for word in lemmas)\n"
      ],
      "metadata": {
        "id": "JAriHz2fQ8h9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(['punkt', 'stopwords', 'averaged_perceptron_tagger', 'wordnet'])\n"
      ],
      "metadata": {
        "id": "Lp9AVGrKTeFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n"
      ],
      "metadata": {
        "id": "OmX6WcViUKz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load preprocessed data\n",
        "df = pd.read_csv('/content/processed_data_spam.csv', encoding='latin-1')\n",
        "# Drop rows with missing 'processed' text\n",
        "df.dropna(subset=['processed'], inplace=True)\n",
        "\n",
        "# Reset index just in case\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Encode labels\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "df['label_encoded'] = label_encoder.fit_transform(df['label'])\n",
        "y = df['label_encoded']\n",
        "# Encode labels (ham = 0, spam = 1)\n",
        "label_encoder = LabelEncoder()\n",
        "df['label_encoded'] = label_encoder.fit_transform(df['label'])\n",
        "y = df['label_encoded']\n",
        "\n",
        "# --- Bag of Words ---\n",
        "count_vectorizer = CountVectorizer()\n",
        "X_bow = count_vectorizer.fit_transform(df['processed'])\n",
        "\n",
        "# --- TF-IDF ---\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(df['processed'])\n",
        "\n",
        "# --- Dimensionality Reduction (Optional) ---\n",
        "# This reduces features to 100 components (you can tweak this number)\n",
        "svd = TruncatedSVD(n_components=100, random_state=42)\n",
        "X_bow_reduced = svd.fit_transform(X_bow)\n",
        "X_tfidf_reduced = svd.fit_transform(X_tfidf)\n",
        "\n",
        "# --- Output shapes ---\n",
        "print(\"Bag of Words shape:\", X_bow.shape)\n",
        "print(\"TF-IDF shape:\", X_tfidf.shape)\n",
        "print(\"Reduced BoW shape:\", X_bow_reduced.shape)\n",
        "print(\"Reduced TF-IDF shape:\", X_tfidf_reduced.shape)\n",
        "\n",
        "# At this point you have:\n",
        "# X_bow           -> Full Bag of Words\n",
        "# X_bow_reduced   -> Dimensionality-reduced BoW\n",
        "# X_tfidf         -> Full TF-IDF\n",
        "# X_tfidf_reduced -> Dimensionality-reduced TF-IDF\n",
        "# y               -> Labels\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHND0q_1egkx",
        "outputId": "abeed68e-441f-402a-adaf-a6fd46729c0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of Words shape: (5562, 7105)\n",
            "TF-IDF shape: (5562, 7105)\n",
            "Reduced BoW shape: (5562, 100)\n",
            "Reduced TF-IDF shape: (5562, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# --- Load and clean data ---\n",
        "df = pd.read_csv('processed_data_spam.csv', encoding='latin-1')\n",
        "df.dropna(subset=['processed'], inplace=True)\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# --- Encode labels ---\n",
        "label_encoder = LabelEncoder()\n",
        "df['label_encoded'] = label_encoder.fit_transform(df['label'])\n",
        "y = df['label_encoded']\n",
        "\n",
        "# --- Define vectorizers ---\n",
        "vectorizers = {\n",
        "    \"TF-IDF\": TfidfVectorizer(),\n",
        "    \"Bag of Words\": CountVectorizer()\n",
        "}\n",
        "\n",
        "# --- Classifier models ---\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(),\n",
        "    \"Naive Bayes\": MultinomialNB(),\n",
        "    \"SVM (Linear)\": LinearSVC()\n",
        "}\n",
        "\n",
        "# --- Run for each vectorizer ---\n",
        "for vec_name, vectorizer in vectorizers.items():\n",
        "    print(f\"\\n====== Using {vec_name} ======\")\n",
        "    X = vectorizer.fit_transform(df['processed'])\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    for model_name, model in models.items():\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        print(f\"\\nðŸ”¹ Model: {model_name}\")\n",
        "        print(\"Accuracy :\", accuracy_score(y_test, y_pred))\n",
        "        print(\"Precision:\", precision_score(y_test, y_pred))\n",
        "        print(\"Recall   :\", recall_score(y_test, y_pred))\n",
        "        print(\"F1 Score :\", f1_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38KcnjTrgxYO",
        "outputId": "fa5ce999-f34b-40cf-f1ca-cd1cb4cfbad1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "====== Using TF-IDF ======\n",
            "\n",
            "ðŸ”¹ Model: Logistic Regression\n",
            "Accuracy : 0.954177897574124\n",
            "Precision: 1.0\n",
            "Recall   : 0.6709677419354839\n",
            "F1 Score : 0.803088803088803\n",
            "\n",
            "ðŸ”¹ Model: Naive Bayes\n",
            "Accuracy : 0.9595687331536388\n",
            "Precision: 1.0\n",
            "Recall   : 0.7096774193548387\n",
            "F1 Score : 0.8301886792452831\n",
            "\n",
            "ðŸ”¹ Model: SVM (Linear)\n",
            "Accuracy : 0.9883198562443846\n",
            "Precision: 1.0\n",
            "Recall   : 0.9161290322580645\n",
            "F1 Score : 0.9562289562289562\n",
            "\n",
            "====== Using Bag of Words ======\n",
            "\n",
            "ðŸ”¹ Model: Logistic Regression\n",
            "Accuracy : 0.9829290206648698\n",
            "Precision: 1.0\n",
            "Recall   : 0.8774193548387097\n",
            "F1 Score : 0.9347079037800687\n",
            "\n",
            "ðŸ”¹ Model: Naive Bayes\n",
            "Accuracy : 0.9784366576819407\n",
            "Precision: 0.8922155688622755\n",
            "Recall   : 0.9612903225806452\n",
            "F1 Score : 0.9254658385093167\n",
            "\n",
            "ðŸ”¹ Model: SVM (Linear)\n",
            "Accuracy : 0.9856244384546271\n",
            "Precision: 0.986013986013986\n",
            "Recall   : 0.9096774193548387\n",
            "F1 Score : 0.9463087248322147\n"
          ]
        }
      ]
    }
  ]
}